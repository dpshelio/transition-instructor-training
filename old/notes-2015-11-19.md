**11:00 UK Time**

* Greg Wilson
* Kai Blin
* Steve Crouch
* Toby Hodges
* Jane Charlesworth
* Fiona Tweedie
* Felipe Bocca
* David Ketcheson
* Peter van Heusden
* Stefano Menegon
* Moritz Neeb
* Joanna Jedrzejewska-Szmek
* Jillian Dunic
* Luciano Issoe
* Joaquín Morís
* Jon Badalamenti

**19:00 UK Time**

* Greg Wilson
* Patrick Chapman
* Jingchao Zhang
* Jennifer Nafziger
* Nada Ghanem
* Patricia Vera Wolf
* Grzegorz Haranczyk
* Tomomi Sunayama
* Holly Murray
* Michelle Craft
* Elita Baldridge
* Christina Koch
* Bérénice Batut
* Lukas Weber
* Karin Lagesen
* Sean Barberie
* Keith Ma
* Skipper Seabo
* Sigrid Klerke
* Nina Overgaard Therkildsen
* Tobin Magle
* Edwin van der Helm
* Tina O'Grady
* Andreas Mueller
* Juan Ugalde
* Bill McClung
* Steve Van Tuyl
* Rochelle Terman
* Jin Choi
* Eric Ma
* Sonia Suarez
* Jackie Milhans
* Melissa Guzman
* Ross Barnowski


* http://coursesource.org: published, peer reviewed undergraduate lessons. Allows academics to get credit - in the form of publications, reputation, citations etc - for something that they often spend a lot of time producing
* How teaching knowledge is actually transferred: http://software-carpentry.org/blog/2015/09/how-teaching-knowledge-is-transferred.html
  * disseminator push (workshops, written down best-practice recommendations) have little or no effect on how people actually teach -
  * 'pull transfer', where people watch teaching and take on the bits that are useful to them are much more useful (n.b. everyone will take on something different)
  * Some things can be transferred in written form (e.g. learning from sheet music), but teaching seems to be learned better by observation
  * Training like this can help provide shared vocabulary + ideas to talk about teaching knowledge and learn more skills yourself
* "What I do with these skills" - lots of cool stories
  * Greg would like to turn some of them into short demo videos
* Before/After stories - Greg interested in using some of your examples in a blog post!  Will email for permission over the weekend.  
  * These stories can help drive where the lessons go and what the learning objectives are.  
* Demotivation: http://jessenoller.com/blog/2015/9/27/a-lot-happens
  * It's a big step that people are starting to talk about this stuff more openly, like the way that people are starting to talk about mental health. Knowing that they're not alone makes it more possible to ask for help. Sharing these stories makes an opening for other people to share their stories. Being open makes it easier for people to let you know if there is a problem in a workshop. It can be very good to start with a story that makes people open to sharing their's.
  * How to fix? Need to recognise that people with the authority, such as the trainer at the front of the room, have the responsibility. Supporting learning is about more than standing up the front talking - need to create a safe environment, where people are comfortable, and able to learn.  

What Happens Next

* this instructor training is a pilot of a new technique
* Single largest complaint about Instructor Training in the past has been that it ends too abruptly
  * people are given the tools/information to read but then left to their own devices to follow up on it
  * they would like more time to get to know other instructors, discuss & share content etc.
* People attending this instructor training and the two-day training in December (7th & 8th) will be put into small groups with a mentor (someone already trained & experienced in SWC Instruction) to go through examples, good practices, answering & discussing questions in more depth for lessons.
* You will then be asked to teach a "mini-lesson", observed by your mentor. A section of an SWC lesson taught to a small (>=1) audience, observed e.g. via video stream. Mentor will give feedback on presentation style, content, etc. and then you will be certified.
  * This is a larger checkout exercise than in the past because people have found that that's what they missed.

How Should We Assess Instructor Training?

* The "mini lesson" should be a good assessment opportunity, have a rubrique? Another possibility is to have scenarios or problems, and to assess the performance
* Submission of new lessons, editing existing ones, engaging in discussion of content
* Engagement with the community in general - blog posts, comments, pull requests, social media
* What's your definition of success? Someone who leads workshops? What about feedback from the learners in their lessons (I'm not sure a mini-lesson would fully measure this)? What about someone who contributes to lessons, but can't/choose not to teach? I think the very broad range of learners we are targeting will make assessment difficult as we don't control for so many human variables.
* There should be SMART objectives for the training, so we know what to assess.  This is difficult because it might involve assessing the students of the students.
* Do some "before" (now) and "after" (January) qualitative analysis (e.g., how confident do you feel teaching x, y, z and why).
* Including some kind of wrap-up survey to go along with the pre-assessment (for the learners)? 
* Improved student feedback from workshops? Improved student feedback from non-SWC teaching would be an indicator that teaching overall is better
* peer review - one instructor comment on the other's performance during the sessions. 
* Try teaching your co-workers before and after the instructor training. Check how their response/evaluation changes.
* Persistance of new instructors continuing to teach? (Do folks teach only one workshop, or do they continue to keep teaching workshops?)
* I still think this is a tricky one, as I haven't seen too many successfull attempts at assessing teaching even in a local setting with lots of students. I think the remote nature of this makes it even harder.
* Students review the workshops, yes? These reviews could presumably be matched with instructors and mapped back to thier training.
* Continuing education opportunities for experienced instructors <- this isn't exactly assessment of the training, but could be indicative of how serious SC/DC is about keeping the quality of instruction high
* send a questionnaire to experienced instructors after they teach a workshop with one of the instructor training students.
* I think multiple metrics is important:
  * Delta in student learning.
  * Peer review.
  * Pre-instructor training assessment vs. post-instructor training assessment of teaching techniques used.
  * 


Open Questions

* How do I deal with a wide range of experiences in the attendees of a workshop e.g. 1/2 class uses Python weekly, other 1/2 has never written a script?
  * http://software-carpentry.org/blog/2015/11/teaching-extreme-ranges.html
  * http://software-carpentry.org/blog/2015/10/pulling-along-those-behind.html
  * Try to use the pre-workshop assessment to avoid getting into this position in the first place - if you can easily identify two groups in the applicants, tell one group that this isn't the workshop for them but that you will run another one, more suited to them, soon.
  * The problem is fundamentally that one group will already have constructed a mental model of the subject, while the other won't.
* How do I sell SWC to PIs who view training as a waste of research time (perhaps help these people construct mental models of why SWC will benefit their research)? Quantitative measures of 'success' would definitely help.
  * Our training will save participants 10-20% of their working time for the rest of their research careers
    * But how is this measured?
  * I know this is just an xkcd, but it actually makes a valid point I think: https://xkcd.com/1205/ +1
* Has anyone looked at how to link SWC to community building? If I organise SWCs at my institution, how do I feed this into building the research and learning community?
  * Try to draw attendees from a community that already knows each other, or at least encourage members of lab groups to sign up together
  * Organizing a social event after the first day can be very valuable to the participants
    * note: give information about this sufficiently ahead of time so that people can organize child care ++yes
  * Certain SWC "hubs" have developed communities around SWC -> see University of Melbourne, Vancouver (UBC + SFU), folks in the Bay Area
  * Some SWC "alumni" have organized local study groups, https://github.com/thehackerwithin
* Are there "badges" for SWC attendees? (e.g. using the open badges platform http://openbadges.org/) yes
  * We do lots of community stuff at UniMelb - we've got badges  - see http://resbaz.github.io/ResBaz-Community/ We hold regular 'hacky hour' drop-ins for workshop attendees to come back, reconnect & practise their skills
* The logistics of setting up and running a workshop http://software-carpentry.org/workshops/operations.html - really good idea to read this, and please let us know if you have any questions
  * one instructor typically takes the lead in terms of setting up the website, helping decide who teaches what
  * reiumbursement (for travel, food, lodging) comes from the institution hosting
* Is there a repository of positive (negative?) testimonials from past students highlighting how SWC has changed their daily practices/improved their computer capabilities?
  * Does SWC maintain a database of student contact info (e.g. emails) so that post-workshop data could be gathered?
  * http://software-carpentry.org/pages/testimonials.html
* How to make connections locally? +1 -- is there a directory of SWC instructors?
  * http://software-carpentry.org/pages/team.html
  * http://software-carpentry.org/pages/join.html
  * A forum maybe? Instead of mailing list. that seems to be an age thing. I much prefer mailing lists :) (I do as well, but as mail lists are not working, something else could be tested)
  * https://swcarpentry.slack.com/messages
* Is is actually possible to create a lesson that caters equally well to every learning style, without the dilution of the overall amount learned becoming too great?
  * at the GOBLET train the trainer workshop we worked on this (guided by Sarah Morgan). We took a topic, provided a theoretical background (theorists), invited students to explore (activists), worked through an example and reflect on it. This is just a sketch but yes it seems possible.
* What is the "standard" way to recruit instructor candidates.
  * often, previous attendees or helpers
* How do we deal with the issue of time commitment - 2 day workshops are a huge time commitment (you mentioned earlier that some are teaching these workshops across 4 half-days - are there other ways to make the time commitment easier to swallow)?
  * for instructors: Basically ask for volunteers, it's up to the instructor if they can make the time.  Can also be a benefit for job hunting!  :)
  * for students: people are interested.  Just be clear up front about how long it will take and that you expect people to attend the whole time.  
* Does SC or DC have experience with running multiple workshops per year at the same institution? How has that gone? Are you usually running these over weekends? during the week?
  * UNL has run four in the past year. Only the last was below capacity (40).
  * University of Wisconsin - Madison does 2-3 a year, of about 30-35 people each time, during the week.  We've done both 2 full days and 4 half days.  They generally fill up fairly quickly.  We have a local pool of instructors (how many? 3-4 fully certified, group of 4-5 with experience), so we're usually organizing in-house
  * Stanford has also done several over a short amount of time. 
  * Berkeley has many workshops per year
* How do you ensure the quality of instructors? The instructors should have more structured knowledge than students.
  * checkout exercises are to ensure that you can teach a group of people, are familiar enough 
  * that said, you don't need to be an expert in the content to teach.  Having a basic mental model is often "good enough"
    * Christina says: see her video from last weekend's retreat on SQL!  (https://plus.google.com/u/1/events/cdp30r9sqkca6vr1ri4pt0uhteo) I've never used/taught sql before.  
* How does SC suggest you approach students who are really struggling with the materials? What do you do when you feel like you've given everything you can for a student and they're still "not getting it"?
  * look at blog posts on diverse learners above
* In how far should we try to convince researchers that SWC will be useful? Some people could improve their research a lot but are too lazy to learn.
  * There is plenty of interest, we don't need to convince anyone.
  * We need to convince people that they can learn this stuff.
* What is the minimum number of workshops per year we should teach to keep up our skills? Can we lose our SC 'licence'?
  * ~1x a year in normal circumstances
  * We cannot lose your licence but you can only vote if you've taught recently.
* How to take SWC lessons and deliver it in a project based style – researchers care more on solving issues related to their research ?
  * Big ask on instructors
  * this has been done - morning of instruction, afternoon of application, foll
* Do we get a SWC vrs DC designation? have we decalred this? Follow up - if i've got one (SC or DC), do i go through the same process (this class) to get the other?

Plus and Minus
* What's the most useful or interesting thing you learned in the past few weeks (from us), and what one thing could we do (or have done) to make this training better?
* Worthwhile: live coding.
* I expected a software/data focus but 
* Learning to and thinking about creating formative assessments (e.g., diagnostic MCQs)
* The whole idea of a mental model.[+1] [+1]
* (+) I really enjoyed listening to Greg talk about learning from a cognitive science / philosophical perspective
* (building a mental model)[+]
* faded example[+]
*  organization &instructors collaboration spirit[+]
*  structuring the lessons in piaza ,and adding material under each week[-]
* Writing the whole content in Eithepad is distracting , specially with using many colors [-]
* 
* (+) So many things, it's hard to rank. I think using smart MCQ for assessments.
* (-) The only thing I did struggle with was Piazza. Blujeans and Etherpad worked well for me, but I missed new topics and such in Piazza. +2 to Piazza being non-intuitive
  * Piazza is especially difficult if you're using it for multiple classes simultaneously - it does not do a good job at all keeping classes separate
* (+)I really liked the homework that forced me think about the topics discussed each week. I was less keen on the teaching motivation survey, though - the results were hard to understand. (-)I'd prefer more opportunities for live-coding.
* (+) How to write learning objectives and use learner profiles to target workshops. And like my neighbour above, definitely I enjoyed having my envelope pushed.
* (-) The Piazza platform was challenging to work with a first. I think we mostly made good use of it towards the end, although it still has problems (e.g. I can't see the latest tags on the nav bar, they don't fit).[+1]
* (+) a lot of "new" ideas to think about, e.g. last time "don't teach Hello World as motivation".
* (-) a bit confusing and easy to get lost in the "lots of material existing".
* (+) learning and thinking more about formative assesment, mental models
* (-) Clear evaluation of homework (by peers). What are the criteria, what could have been done better
* (+) The faded examples were a really great idea - will use these extensively in teaching from now on. +1
* (-) I would have liked an opportunity to get to know a little more about the people involved in the training e.g. the other trainee instructors. As part of our application, we had to tell you a little about the work that we do, and homework 0 allowed us to tell a little about ourselves, but I would have liked a short session to actually say hello & maybe do some ice-breaking over the video conferencing. (Then again, some people might have hated having to do this!)
* Most useful: Reflection upon teaching skills and attitudes. Chance to improve: More chances to practice these skills on real situations during training
* I liked the introductory material on learning theory - novice vs. competent vs. expert[+1]
* Actually practicing a live coding explanation with feedback was the most useful
* + live coding exercise was challenging, teaching was not natural with little not enough grasp of the material. 
* - more experience in workshops, maybe more commented video recordings of lessons.
* (+) I love the live coding session, because I
* - there are better methods of titling your assigments to people can find them. Ex
* (+) All of the discussion about pedagogy in the first few classes was new and fascinating to me - lots of food for thought
* (+) Having had to live code.
* (+) Teaching us findings from learning pyschology
* (+) your stories about demotivational examples(
* (+) the idea of mental models being the difference between novices and others
* (-) some of the stuff about teaching and learning styles I'm not sure how to use in order to improve
* (+) that rough-looking stuff gets the most honest feedback was the most surprising insight
* (-) that we did not have more chances to meet each other in video/audio :)
* (+) I really like the philosophy behind SW/data carpentry. It was great that we spent so much time on learning psych and instructional design
* (-) the online format was good for me logistically, but I think the in person interactions would have been beneficial
* (+) Pairing up to practice mini lessons and I enjoyed the teching philosophy and styles in general
* (-) More specific topics related to coding would be nice
* (+) I learn some much really interesting and usefull things (mental model, MCQ, learning objectives, ...) and also many "techniques"
* (-) Sometimes the barrier language (not being English native)
* (+) all the stuff on teaching ... most of what we covere
