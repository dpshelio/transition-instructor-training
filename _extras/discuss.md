---
layout: page
title: Discussion
permalink: /discuss/
---
This training course only a start.
If you'd like to help us make it better,
we would welcome additions discussing:

*   how education research is done: qualitative studies, quantitative studies, and comparison studies
*   Dewey, Piaget, Vygotsky, Freire (or, key figures in 100 words each)
*   the history of distance education (or, everything old is new again)
*   MOOCs
*   computer-based homework/teaching systems
*   other forms of assessment
*   an overview of research on novice programmers
*   problem-solving skills overview
*   setting expectations in the classroom
*   promoting effective study habits

We would also appreciate additions to this list of things we *don't* do,
and explanations of why not:

peer instruction
:   This powerful teaching method has been proven effective,
    but we are already asking workshop participants to assimilate a lot of new things,
    and picking up a new learning technique while learning the basics of coding and data wrangling
    seems too much to ask.

certification
:   Many people have asked us to certify workshop participants in the same way that we certify instructors,
    but any meaningful certification process would require a lot of resources to set up and run.

## Why Do(n't) We Teach X?

Workshop attendees and trainee instructors often ask why we don't teach
high-performance computing, machine learning, Perl, or a long list of other topics.
Our answer is that as with every curriculum,
the question is not, "What would we like to add?"
but, "What are we willing to take out in order to make room?"
We believe our core topics are the absolute minimum that researchers need to know
in order to work efficiently and reproducibly.
More importantly,
we don't know what we could take out to make space for something else.

One thing we *do* know is that we do not wish to become embroiled in debates
over the relative merits of different languages or operating systems.
No one has ever demonstrated that R programmers are more productive than Python programmers,
and proficient users of Windows seem just as productive as equally-proficient users of Unix.
If a learner asserts that their favorite tool is better than alternatives in some way,
ask them for their data;
if they don't have any,
point out as gently as possible that we're supposed to be scientists,
and that if we want politicians, business leaders, and the general public
to pay attention to our findings on climate change and drug-resistant diseases,
it behooves us to try to meet those same standards ourselves.

> ## Evidence and Its Absence
>
> As far as is practical,
> our teaching methods are based on the best available evidence.
> We wish we could say the same about our content,
> but very little research has been done on what researchers actually use
> and what impact it has on productivity.
> An example of what we wish existed
> is [this summary][stefik-summary] by Stefik et al
> of empirical research on the usability of programming langauges
> (while [this full-length paper][stefik-paper] gives an idea of what's possible).
{: .callout}

## Why We're Not a MOOC

> If you use robots to teach, you teach people to be robots.

This difference between what novices are doing when they learn,
and what competent practitioners are doing,
is one of the reasons we have stopped trying to teach via recorded video
with auto-graded drill exercises.
Any recorded content is as ineffective for most learners as broadcast television,
or as a professor standing in front of 400 people in a lecture hall,
because neither can intervene to clear up specific learners' misconceptions.
Some people happen to already have the right conceptual categories for a subject,
or happen to form them correctly early on;
these are the ones who stick with most massive online courses,
but many discussions of the effectiveness of such courses
ignore this survivor bias.
